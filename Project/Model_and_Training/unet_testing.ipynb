{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making of the neural network, training of the neural network, and testing on various model happens here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        #input: 1x256x3\n",
    "        self.e11 = nn.Conv1d(3, 64, kernel_size=3, padding=1) # output: 1x256x64\n",
    "        self.e12 = nn.Conv1d(64, 64, kernel_size=3, padding=1) # output: 1x256x64\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2) # output: 1x128x64\n",
    "\n",
    "        # input: 1x128x64\n",
    "        self.e21 = nn.Conv1d(64, 128, kernel_size=3, padding=1) # output: 1x128x128\n",
    "        self.e22 = nn.Conv1d(128, 128, kernel_size=3, padding=1) # output: 1x128x128\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2) # output: 1x64x128\n",
    "\n",
    "        # input: 1x64x128\n",
    "        self.e31 = nn.Conv1d(128, 256, kernel_size=3, padding=1) # output: 1x64x256\n",
    "        self.e32 = nn.Conv1d(256, 256, kernel_size=3, padding=1) # output: 1x64x256\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2) # output: 1x32x256\n",
    "\n",
    "        # input: 1x32x256\n",
    "        self.e41 = nn.Conv1d(256, 512, kernel_size=3, padding=1) # output: 1x32x512\n",
    "        self.e42 = nn.Conv1d(512, 512, kernel_size=3, padding=1) # output: 1x32x512\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=2, stride=2) # output: 1x16x512\n",
    "\n",
    "        # input: 1x16x512\n",
    "        self.e51 = nn.Conv1d(512, 1024, kernel_size=3, padding=1) # output: 1x16x1024\n",
    "        self.e52 = nn.Conv1d(1024, 1024, kernel_size=3, padding=1) # output: 1x16x1024\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose1d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv1d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.d12 = nn.Conv1d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose1d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv1d(512, 256, kernel_size=3, padding=1)\n",
    "        self.d22 = nn.Conv1d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose1d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv1d(256, 128, kernel_size=3, padding=1)\n",
    "        self.d32 = nn.Conv1d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose1d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n",
    "        self.d42 = nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv1d(64, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = relu(self.e11(x))\n",
    "        # print(\"xe11: \", xe11.shape)\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        # print(\"xe12: \", xe12.shape)\n",
    "        xp1 = self.pool1(xe12)\n",
    "        # print(\"xp1: \", xp1.shape)\n",
    "\n",
    "        xe21 = relu(self.e21(xp1))\n",
    "        # print(\"xe21: \", xe21.shape)\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        # print(\"xe22: \", xe22.shape)\n",
    "        xp2 = self.pool2(xe22)\n",
    "        # print(\"xp2: \", xp2.shape)\n",
    "\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        # print(\"xe31: \", xe31.shape)\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        # print(\"xe32: \", xe32.shape)\n",
    "        xp3 = self.pool3(xe32)\n",
    "        # print(\"xp3: \", xp3.shape)\n",
    "\n",
    "        xe41 = relu(self.e41(xp3))\n",
    "        # print(\"xe41: \", xe41.shape)\n",
    "        xe42 = relu(self.e42(xe41))\n",
    "        # print(\"xe42: \", xe42.shape)\n",
    "        xp4 = self.pool4(xe42)\n",
    "        # print(\"xp4: \", xp4.shape)\n",
    "\n",
    "        xe51 = relu(self.e51(xp4))\n",
    "        # print(\"xe51: \", xe51.shape)\n",
    "        xe52 = relu(self.e52(xe51))\n",
    "        # print(\"xe52: \", xe52.shape)\n",
    "        \n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        # print(\"xu1: \", xu1.shape)\n",
    "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "        # print(\"xu11: \", xu11.shape)\n",
    "        xd11 = relu(self.d11(xu11))\n",
    "        # print(\"xd11: \", xd11.shape)\n",
    "        xd12 = relu(self.d12(xd11))\n",
    "        # print(\"xd12: \", xd12.shape)\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        # print(\"xu2: \", xu2.shape)\n",
    "        xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "        # print(\"xu22: \", xu22.shape)\n",
    "        xd21 = relu(self.d21(xu22))\n",
    "        # print(\"xd21: \", xd21.shape)\n",
    "        xd22 = relu(self.d22(xd21))\n",
    "        # print(\"xd22: \", xd22.shape)\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        # print(\"xu3: \", xu3.shape)\n",
    "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "        # print(\"xu33: \", xu33.shape)\n",
    "        xd31 = relu(self.d31(xu33))\n",
    "        # print(\"xd31: \", xd31.shape)\n",
    "        xd32 = relu(self.d32(xd31))\n",
    "        # print(\"xd32: \", xd32.shape)\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        # print(\"xu4: \", xu4.shape)\n",
    "        xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "        # print(\"xu44: \", xu44.shape)\n",
    "        xd41 = relu(self.d41(xu44))\n",
    "        # print(\"xd41: \", xd41.shape)\n",
    "        xd42 = relu(self.d42(xd41))\n",
    "        # print(\"xd42: \", xd42.shape)\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = torch.Tensor(1, 1, 3, 256)\n",
    "# model = UNet(3)\n",
    "# output = model(input[0])\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the alphabets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "alpha = {}\n",
    "dirty_alpha = {}\n",
    "with open('../Input_Representation/data/other/alphabet.pkl', 'rb') as f:\n",
    "    alpha = pickle.load(f)\n",
    "with open('../Input_Representation/data/other/dirty_alphabet.pkl', 'rb') as f:\n",
    "    dirty_alpha = pickle.load(f)\n",
    "\n",
    "# new_alpha = {}\n",
    "# for key, value in alpha.items():\n",
    "#     print(np.mean([len(x) for x in value[0]]))\n",
    "# new_dirty_alpha = {}\n",
    "# for key, value in dirty_alpha.items():\n",
    "#     print(np.mean([len(x) for x in value[0]]))\n",
    "\n",
    "\n",
    "# for key in alpha.keys():\n",
    "#     print(key, alpha[key], \"\\n\")\n",
    "\n",
    "# for key in dirty_alpha.keys():\n",
    "#     print(key, alpha[key], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the dataset and making the Dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, alpha, dirty_alpha, transform=None):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        for key, value in alpha.items():\n",
    "            if key in dirty_alpha:\n",
    "                self.data.append(value)\n",
    "                self.labels.append(dirty_alpha[key])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "dataset = MyDataset(alpha, dirty_alpha)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=1)\n",
    "dataiter = iter(dataloader)\n",
    "# data = dataiter.next()\n",
    "# features, labels = data\n",
    "# print(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/luuk-laptop/miniconda3/envs/project/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/luuk-laptop/miniconda3/envs/project/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/luuk-laptop/miniconda3/envs/project/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/luuk-laptop/miniconda3/envs/project/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/luuk-laptop/miniconda3/envs/project/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/luuk-laptop/miniconda3/envs/project/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 138, in collate\n    raise RuntimeError('each element in list of batch should be of equal size')\nRuntimeError: each element in list of batch should be of equal size\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/luuk-laptop/Documents/Scriptie_2024/Project/Model_and_Training/unet_testing.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/luuk-laptop/Documents/Scriptie_2024/Project/Model_and_Training/unet_testing.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(dataiter)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/luuk-laptop/Documents/Scriptie_2024/Project/Model_and_Training/unet_testing.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# data = dataset[0]\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/luuk-laptop/Documents/Scriptie_2024/Project/Model_and_Training/unet_testing.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m features, labels \u001b[39m=\u001b[39m data\n",
      "File \u001b[0;32m~/miniconda3/envs/project/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/project/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/project/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/project/lib/python3.10/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/luuk-laptop/miniconda3/envs/project/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/luuk-laptop/miniconda3/envs/project/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/luuk-laptop/miniconda3/envs/project/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/luuk-laptop/miniconda3/envs/project/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/luuk-laptop/miniconda3/envs/project/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/luuk-laptop/miniconda3/envs/project/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 138, in collate\n    raise RuntimeError('each element in list of batch should be of equal size')\nRuntimeError: each element in list of batch should be of equal size\n"
     ]
    }
   ],
   "source": [
    "data = next(dataiter)\n",
    "# data = dataset[0]\n",
    "features, labels = data\n",
    "print(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
